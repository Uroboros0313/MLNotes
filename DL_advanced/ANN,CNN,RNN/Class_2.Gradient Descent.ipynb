{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Regression的第三步，需要衡量Model的优劣，需要定义一个函数的函数--Loss function   \n",
    "\n",
    "$$\\theta ^* = arg \\mathop{min}\\limits_{\\theta} L(\\theta)$$\n",
    "$L:loss function$     \n",
    "$\\theta :parameters$    \n",
    "\n",
    "假设$\\theta$有两个变量$\\{ \\theta_1,\\theta_2\\}$    \n",
    "随机选取一个$\\theta ^0= \\left [\\begin{aligned} \\theta _1^0\\\\ \\theta _2^0 \\end{aligned} \\right ]$开始,\n",
    "有$\\left [\\begin{aligned} \\theta _1^1\\\\ \\theta _2^1 \\end{aligned} \\right ]=\n",
    "\\left [\\begin{aligned} \\theta _1^0\\\\ \\theta _2^0 \\end{aligned} \\right ]-\n",
    "\\eta \\left [\\begin{aligned} \\partial L(\\theta _1^0)/\\partial \\theta _1 \\\\ \\partial L(\\theta _2^0)/\\partial \\theta _1 \\end{aligned} \\right ]$,\n",
    "多次迭代直到找到optimal，也可以写为：\n",
    "$$\\nabla L(\\theta) = \\left[ \\begin{aligned} \\partial L(\\theta_1)/ \\partial(\\theta_1) \\\\ \\partial L(\\theta_2)/ \\partial(\\theta_2) \\end{aligned} \\right ]$$\n",
    "\n",
    "$$\\theta^1 =\\theta^0- \\eta \\nabla L(\\theta ^0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Tunning the learning rate\n",
    "\n",
    "$\\eta$过大:会导致直接越过optimal    \n",
    "$\\eta$过小:速度过慢    \n",
    "\n",
    "在参数维度大于3时，无法看出gradient descent的图形，所以需要绘制出损失与迭代次数的图象    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Class.Gradient_Descent_1.PNG\" width=600 height=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Class.Gradient_Descent_2.PNG\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Adaptive Learning Rate    \n",
    "\n",
    "- 在迭代初期，通常距离目标值较远，因此会使用较大的学习率   \n",
    "- 当参数迭代更新一定次数后(After several epochs)，选择更小的学习率   \n",
    "- $E.g.\\; 1/t\\; decay:\\eta ^t = \\eta /\\sqrt{t+1}$\n",
    "- 一个学习率难以适应多个参数，因此给不同的参数不同的学习率----$Adagrad$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad      \n",
    "- 将每个参数的$\\eta$除以之前算出的微分值的root mean square（均方根）\n",
    "$$\\theta^{t+1} =\\theta^t- \\frac{\\eta ^t}{\\sigma ^t} \\nabla L(\\theta ^t)$$\n",
    "\n",
    "其中：    \n",
    "$\\eta ^t = \\eta /\\sqrt{t+1}$       \n",
    "\n",
    "$\\sigma ^t=\\sqrt{\\frac{1}{t+1} \\sum ^t_{i=0}(g^i)^2}$\n",
    "\n",
    "\n",
    "- **Question**:\n",
    "1. 计算出的梯度越小（远小于0时），$\\sigma ^t$会导致参数更新更快，但与梯度越大，参数更新越快矛盾    \n",
    "  学习率/步长的合理值，不仅与一次微分有关，也与二次微分有关；       \n",
    "  想象一个如下图所示的二参数的优化，由于不同方向的梯度不同，可能导致函数值较大的地方实际上离极值点更近，因此需要利用二次微分的值修正     \n",
    "  可以理解成利用均方根来模拟二次微分的值（二次微分较大，变化较大，均方根较大---均方根对离群值敏感）\n",
    "\n",
    "\n",
    "2. 迭代次数较多时，时间复杂度与空间复杂度"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Class.Gradient_Descent_3.PNG\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Stochastic Gradient Descent(随机梯度下降)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent:$L= \\sum(\\hat{y^i}-(b+ \\sum w^ix^i))^2$      \n",
    "Stochastic Gradient Descent:$L= (\\hat{y^k}-(b+ \\sum w^kx^k))^2$  (随机sample一个$x_n$，每次迭代采用一个样本进行参数更新)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SGD收敛速度比BGD要快：     \n",
    "\n",
    "这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）；而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被更新（迭代）30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W 次。\n",
    "\n",
    "从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Class.Gradient_Descent_4.PNG\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于样本值中值域相差较大的$x^i$，可能会对梯度下降产生一定的影响（即不同的属性对梯度下降的影响程度不同）    \n",
    "如下图所示，梯度下降会偏向梯度较大的维度的等高线法线方向下降    \n",
    "可以采用以下方式：     \n",
    "$$x_i^{r(new)} = \\frac{x_i^r-m_i}{\\sigma _i}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Class.Gradient_Descent_6.PNG\" width=600 height=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Class.Gradient_Descent_5.PNG\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Taylor series(泰勒级数）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义：如果在点x=x0具有任意阶导数，则如下幂级数为函数在$x_0$处的泰勒级数展开（在该点对函数的逼近）\n",
    "$$\\sum_{i=0}^\\infty \\frac{f^{(i)}(x_0)}{n!}(x-x_0)^i$$     \n",
    "考虑只取一阶导数的情况，抛弃高阶无穷小   \n",
    "$$h(x,y) = h(x_0,y_0)+\\frac{\\partial h(x_0,y_0)}{\\partial x}(x-x_0)+\\frac{\\partial h(x_0,y_0)}{\\partial y}(y-y_0)$$    \n",
    "考虑梯度下降的图象，在当前点$(a.b)$的一个邻域内需要选择一个方向进行梯度下降（即让损失函数最小的方向）   \n",
    "其中，损失函数等于$L(\\theta) = L(a,b)+\\frac{\\partial L(a,b)}{\\partial \\theta_1}(\\theta_1-a)+\\frac{\\partial L(a,b)}{\\partial \\theta_2}(\\theta_2-b)$       \n",
    "令$ L(a,b)=s,\\; \\frac{\\partial L(a,b)}{\\partial \\theta_1}=u,\\; \\frac{\\partial L(a,b)}{\\partial \\theta_2}=v$    \n",
    "$$L(\\theta)=s+u(\\theta_1-a)+v(\\theta_2-b)=s+u\\Delta \\theta_1+v\\Delta \\theta_2$$   \n",
    "当邻域范围足够小(泰勒级数能够很好的逼近原函数时）,To minimize $L(\\theta),(\\Delta \\theta_1,\\Delta \\theta_2)=-\\eta(u,v)$   \n",
    "<img style=\"float: center;\" src=\"resource_pic/Class.Gradient_Descent_7.PNG\" width=600 height=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Class.Gradient_Descent_8.PNG\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## More limitations in Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 梯度下降法在迭代时，会设置一个精度作为迭代停止的标志,因此会导致在Plateau的更新速度很缓慢\n",
    "2. 鞍点的一次导数等于零，二次导数换正负符号（极值点要求二阶导数在该点不等于0/不换符号）；可能会因为一次导数等于0的原因被判断为极值点\n",
    "3. 局部最优解"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Class.Gradient_Descent_9.PNG\" width=600 height=600>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
