{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK3\n",
    "## Seq2Seq\n",
    "机器翻译任务——将一串法语翻译成英语     \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_1.PNG\" width=500 height=500>     \n",
    "\n",
    "**定义两个网络:Encoder与Decoder**    \n",
    "**Encoder**:接受输入序列，输出一个向量表征这个序列(编码)     \n",
    "**Decoer**:将Encoder的输出做为输入，每步输出一个值——直到输出序列的结尾。    \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_2.PNG\" width=500 height=500>       \n",
    "\n",
    "语言模型与机器翻译的差别:     \n",
    "- 语言模型使用是个随机输入与**随机采样**来形成一个文本序列。\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_3.PNG\" width=500 height=500>\n",
    "- 机器翻译的输出部分接受的**不是随机输入**，而是一个**编码向量**。并且与语言模型不同的是，机器翻译追求$P(y^{<1>},y^{<2>}...y^{<T_y>}|x)$的最优值，因此在每次输出后不是采用**随机采样**，而是**集束搜索**       \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_4.PNG\" width=500 height=500>              \n",
    "\n",
    "## Beam Search（集束搜索）                \n",
    "\n",
    "1. 第一步:集束搜索在Decoder每次输出时会考虑可能性最大的$n$个单词，$n$是集束宽度($Beam \\ width$)\n",
    "2. 第二步:由于第一步搜索出了$n$个词语，得到第一个输出的概率$P(y^{<1>}|x)$;根据第一步$n$个输出，继续估计下一个词语，得到概率值$P(y^{<1>},y^{<2>}|x)$，这样一共会有$Beam\\ width*Number\\ of\\ words$个词语，再根据条件概率选出其中最大的$n$个词语。        \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_5.JPG\" width=500 height=500>\n",
    "\n",
    "### 改进集束搜索的技巧       \n",
    "\n",
    "1. 集束搜索中，每个词语的概率值都是小于1，因此连乘可能导致数值下溢——解决方法:取对数   \n",
    "\n",
    "$$P(y^{<1>},y^{<2>}...y^{<T_y>}|x) = P(y^{<1>}|x)P(y^{<2>}|x,y^{<1>})....P(y^{<T_y>}|x,y^{<1>},...y^{<T_y-1>})$$            \n",
    "\n",
    "$$argmaxP(y^{<1>},y^{<2>}...y^{<T_y>}|x)$$    \n",
    "\n",
    "$$\\Rightarrow \\underset{y}{arg\\ max}\\prod P(y^{<t>}|x,y^{<1>}...y^{<t-1>})$$    \n",
    "\n",
    "$$\\Rightarrow \\underset{y}{arg\\ max}\\sum log(P(y^{<t>}|x,y^{<1>}...y^{<t-1>}))$$       \n",
    "\n",
    "2. 集束搜索中，最优化函数倾向于更短的句子——解决方法:归一化(除以句子长度或句子长度的$\\alpha(=0.7,usually)$ 次幂)      \n",
    "\n",
    "$$\\frac{1}{T_y^\\alpha} P(y^{<1>},y^{<2>}...y^{<T_y>}|x)$$    \n",
    "\n",
    "\n",
    "### 集束搜索的误差分析\n",
    "\n",
    "机器翻译句子:@@jane is a pussy boy@@@@@@—— $\\hat y$              \n",
    "人类翻译句子:##jane is a bad boy######—— $y^*$ \n",
    "\n",
    "利用RNN计算两个句子的概率$P(y\\hat|x)$与$P(y^* |x)$     \n",
    "那么存在两种情况    \n",
    "1.$P(y\\hat|x)>P(y^* |x)$     \n",
    "网络结构出问题    \n",
    "2. $P(y\\hat|x)<P(y^* |x)$    \n",
    "$Beam\\ Search$出问题\n",
    "\n",
    "## BLEU   \n",
    "```\n",
    "翻译一个法语句子，有两个参考的人工翻译  \n",
    "\n",
    "Reference 1: The cat is on the mat.\n",
    "Reference 2:There is a cat on the mat.\n",
    "```\n",
    "\n",
    "Bleu的原理:机器翻译的结果接近人工的参考翻译结果——看生成的词是否出现在参考翻译中。    \n",
    "\n",
    "```\n",
    "percision:机器翻译中的单词出现在参考翻译中的个数/总单词个数   \n",
    "modified:机器翻译中的单词出现在参考翻译中的个数/总单词个数，\n",
    "      每个单词有个计分上限，比如\"the\"，在翻译1中出现2次，翻译2中出现1次，计分上限为2\n",
    "```\n",
    "\n",
    "\n",
    "### n-grams的Bleu   \n",
    "```\n",
    "Example: \n",
    "Reference 1: The cat is on the mat.\n",
    "Reference 2: There is a cat on the mat.\n",
    "MT output: The cat the cat on the mat.\n",
    "         count   count_clip\n",
    "the cat    2       1\n",
    "cat the    1       0\n",
    "cat on     1       1\n",
    "on the     1       1\n",
    "the mat    1       1\n",
    "\n",
    "BLEU = 4/6\n",
    "```\n",
    "\n",
    "对于**n-grams**而言:   \n",
    "\n",
    "$$P_n = \\cfrac{\\sum_{n-grams\\in \\hat y }Content_{clip} (n-grams)}{\\sum_{n-grams\\in \\hat y}Content(n-grams)}$$\n",
    "\n",
    "### Combined Bleu Scores      \n",
    "\n",
    "$$(BP)exp(\\frac{1}{4}\\sum_{n=1}^4P_n),BP-brief\\ penalty(简短惩罚)$$    \n",
    "\n",
    "$$\n",
    "BP =\\begin{cases}  \n",
    "1&,if MT_{output\\ length} > reference_{output\\ length}\\\\\n",
    "exp(1 — MT_{output\\ length}/reference_{output\\ length})&,otherwise\n",
    "\\end{cases} \n",
    "$$    \n",
    "\n",
    "\n",
    "## 注意力模型     \n",
    "    \n",
    "\n",
    "当人在翻译句子的时候，不会一次性翻译一页，而是一段一段翻译。    \n",
    "在Encoder-Decoder模型当中，长段落会导致编码在压缩向量时产生损失，相比于短段落BLEU分值低。    \n",
    "\n",
    "考虑下面这样的一个模型    \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_6.png\" width=500 height=500>    \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_7.png\" width=500 height=500>\n",
    "\n",
    "\n",
    "## 语音辨识    \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_8.png\" width=500 height=500>    \n",
    "\n",
    "## Trigger word     \n",
    "不平衡数据集的解决方法——听到trigger word之后持续输出一段时间的1    \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_9.png\" width=500 height=500>  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
