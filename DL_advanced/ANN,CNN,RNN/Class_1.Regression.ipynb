{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression:输出值是数值型    \n",
    "Binary Classification:二分类    \n",
    "Multi-class Classification:多类别分类     \n",
    "Generation:生成   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised Learning:给机器一部分训练资料(Labeled Data---提供输入与理想输出),生成function    \n",
    "Loss:损失函数(利用梯度下降法可以最小化损失函数)   \n",
    "\n",
    "- Reinforcement Learning:强化学习不同于监督学习,主要表现在强化信号上,强化学习中环境提供的强化信号是对产生动作好坏的一种的评价,而不是如何产生正确的动作(监督学习)\n",
    "\n",
    "- Network architecture    \n",
    "给定一个函数的搜寻范围\n",
    "\n",
    "- Meta Learning\n",
    "教会机器学习如何学习  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP1:MODEL     \n",
    "STEP2:Goodness of Function    \n",
    "STEP3:Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Gradient Descent(梯度下降)   \n",
    "$$w^* = arg min_wL(w)$$ \n",
    "- **需要可微分**   \n",
    "1.(randomly) pick an initial value $w^0$    \n",
    "2.compute $ \\frac {dL}{dw} |_{w=w^0}$   \n",
    "  Negative----increase w    \n",
    "  Positive----decrease w  \n",
    "  $$w^0- \\eta \\frac {dL}{dw} |_{w=w^0}  \\rightarrow w^1$$    \n",
    "  $\\eta$:learning rate(较大时，学习速度较快)    \n",
    "  经过迭代梯度下降之后，会到达一个local optimal上(not global optimal)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOW about two parameters？   \n",
    "对于多个变量而言，在同一点分别求取偏微分    \n",
    "$$\\nabla L=\n",
    "\\left [\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial w}\\\\\n",
    "\\frac{\\partial L}{\\partial b}\n",
    "\\end{aligned}\n",
    "\\right ] \\rightarrow gradient$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Gradient Descent的问题    \n",
    "存在local optimal和global optimal  \n",
    "<img style=\"float: center;\" src=\"resource_pic/Gradient_descent_1.PNG\" width=600 height=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在linear regression中不存在两者的区别，如图：    \n",
    "<img style=\"float: center;\" src=\"resource_pic/Gradient_descent_2.PNG\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "    做regression时,如果选择更加复杂的model,他可能包含了底层级的model,代表可以找到一个model在训练集上的误差要比底层及的model低,但在测试集上不一定会有更好的结果"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Gradient_descent_3.PNG\" width=600 height=600>\n",
    "![Gradient_descent_3.PNG](attachment:Gradient_descent_3.PNG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Gradient_descent_4.PNG\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Regularization\n",
    "重新定义loss function   \n",
    "$$L= \\sum^n(\\hat{y}-(b+ \\sum w_ix_i))^2 + {\\color{Red}{\\lambda\\sum(w_i)^2}}$$   \n",
    "> - 认为具有小的$w_i$的损失函数更好,具有更小的$w_i$的model的输出对于输入更不敏感(不需要考虑bias,因为对model的平滑程度没有影响),因此会得到一个更加平滑的model function,对于含有一部分噪声的输入而言,输出受到的影响会更小\n",
    "> - 当考虑的$\\lambda$越大,相应的对应训练集的误差会越大($\\lambda$代表更少地考虑训练集的误差)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # BASIC CONCEPT OF ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ERROR的来源    \n",
    "  1.**due to \"bias\"**     \n",
    "  2.**due to \"variance\"**     \n",
    "\n",
    "- Bias and Variance of Estimator:    \n",
    "  对于变量$X$而言,$\\{ X_1,X_2 \\dots X_N\\}$ 是总体中的一组样本   \n",
    "  1.对于$\\mu$的估计\n",
    "  $$\\bar{X} = \\frac{1}{N} \\sum _i^N X_i \\ne \\mu$$\n",
    "  $$E[\\bar{X}] =E[ \\frac{1}{N} \\sum _i^N X_i] = \\frac{1}{N} \\sum _i^N E[X_i] = \\mu$$  \n",
    "  2.对于$\\sigma^2$的估计\n",
    "  $$Var[\\bar{X}] =\\frac{\\sigma ^2}{N}$$\n",
    "  $$s^2=\\frac{1}{N}\\sum _i^N(X_i-\\bar{X})^2$$\n",
    "  $$E[s^2]=\\frac{N-1}{N}\\sigma ^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 误差(ERROR):    \n",
    "  1. $E[f^*]=\\bar f$与中心$\\mu$存在偏差,这是由于Bias造成的误差(系统误差)     \n",
    "  2. 由于较大的方差(Variance)也会造成较大的误差（随机误差）    \n",
    "\n",
    "无偏估计不代表好的估计，有时候方差小的有偏估计优于方差大的估计   \n",
    "Variance:对于比较简单的模型,方差较小(可能有偏,但是较为集中):因为模型会更少地受到训练集的影响(极端例子$f(x)=c$)    \n",
    "Bias:对于比较简单的模型,偏差较大,对于复杂模型是偏差较小的(复杂模型包含的空间更大,能够找到更好符合$\\hat{f}$的function)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Error_1.PNG\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Underfitting(cannot fit the training data):导致Bias过大---redesign model   \n",
    "  1. Add more features as input   \n",
    "  2. A more complex model   \n",
    "\n",
    "\n",
    "- Overfitting(fit the model,have large variance)   \n",
    "  1. More data(有效且不会导致Bias变大,但可能会出现实际无法收集足够多数据的情况下)\n",
    "  2. Regularization(可能导致Bias变大)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection    \n",
    "**NOT TO DO**   \n",
    "- 通过Training Set训练出的模型中,在Testing Set(测试集,测试集本身可能有偏,public set)中选择Model时在实际的Testing Set(真实情况,private set)中表现不佳   \n",
    "\n",
    "**Cross Validation**       \n",
    "- 交叉验证:将Training Set分为两部分---Training Set(用来建立模型)与Validation Set(用来选择模型)     \n",
    "- N-fold Cross Validation:将数据集分为N-fold 选取每一折当作一次的Validation Set,计算Ave Error     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
