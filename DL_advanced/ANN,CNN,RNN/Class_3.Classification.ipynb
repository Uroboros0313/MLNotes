{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 1. Classification  \n",
    "   \n",
    "   下标（vector中的第i个变量）     \n",
    "   上标（第i个vector）\n",
    "   1. 先验概率（prior probability）:是指根据以往经验和分析得到的概率       \n",
    "   2. 后验概率（posterior probability）:事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小\n",
    "$$x\\Rightarrow \\mathbf{Funtion} \\Rightarrow \\; Class\\; n$$    \n",
    "\n",
    "## Ideal Alternative    \n",
    "- Function(Model)    \n",
    "  $$\\begin{cases} g(x) >0&Output=class\\;1 \\\\ else &Output=class2\\;2 \\end{cases}$$\n",
    "  \n",
    "- Loss function      \n",
    "  $\\delta(function)\\Rightarrow$括号内的函数为真计1，否则计0    \n",
    "  The number of times f get incorrect results on training data (无法用Gradient descent)   \n",
    "  $$L(f) = \\sum_n\\delta(f(x^n)\\neq \\hat{y}^n)$$\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯（生成模型-generative model）      \n",
    "当存在两个类别以及**一个给定的实例**，类别1中抽到给定实例$x$的概率是$P(x|C_1)$，类别2是$P(x|C_2)$；     \n",
    "**随机抽取**一个实例，抽到类别1与2的概率分别是$P(C_1)，P(C_2)$    \n",
    "\n",
    "那么给定的实例$x$属于某个类别的概率为$P(C_1|x)=\\cfrac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2) }$     \n",
    "而对于实例而言，是带有一个特征向量（feature vector）代表他的属性;    \n",
    "通过训练集中数据的该特征向量，可以拟合出一个与特征向量维度相同的分布来计算类别$\\;i\\;$中抽到给定实例$x$的概率$P(x|C_i)$  \n",
    "<img style=\"float: center;\" src=\"resource_pic/Class.Classification_1.png\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高斯分布为例         \n",
    "假设我们有随机向量$\\vec{Z} = [Z_ 1，\\cdots, Z_ n]^T$, 其中$Z_ i \\sim \\mathcal{N}(0， 1)(i=1,\\cdots, n)$,     \n",
    "且$Z_ i,Z_j(i,j=1，\\cdots, n \\wedge ii\\neq j)'$彼此独立, 即随机向量中的每个随机变量$Z_{i}$都服从标准高斯分布且两两彼此独立.      \n",
    "则由(4)与独立随机变量概率密度函数之间的关系, 我们可得随机向量$\\vec{Z} = [Z_ 1, \\cdots,Z_ n]^\\top$的联合概率密度函数为:      \n",
    "$$\\begin{align} \n",
    "p(z_1, \\cdots, z_n) & = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot (z_i)^2}\\\\ \n",
    "{}& = \\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot (Z^T Z)} \\\\\n",
    "1 & = \\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} p(z_1, \\cdots, z_n) \\ dz_1 \\cdots dz_n \n",
    "\\end{align}$$      \n",
    "\n",
    "我们称随机向量$\\vec{Z} \\sim \\mathcal{N}(\\vec{0}, \\mathbf{I})$, 即随机向量服从均值为零向量, 协方差矩阵为单位矩阵的高斯分布. 在这里, 随机向量$\\vec{Z}$的协方差矩阵是$Conv(Z_i, Z_j), i, j = 1, \\cdots, n\\;$组成的矩阵, 即\n",
    "\n",
    "$$\\begin{align} [Conv(Z_i, Z_j)]_{n \\times n} &= \\mathbf{E}[(Z - \\vec{\\mu})(Z - \\vec{\\mu})^\\top] \\\\ &= \\mathbf{I}\\end{align}$$      \n",
    "\n",
    "- 普通的随机向量X（**每个随机变量之间不独立**）       \n",
    "$$p(x_1, \\cdots, x_n) = \\cfrac{1}{(2 \\pi)^{\\frac{n}{2}} \\left| \\Sigma \\right|^{\\frac{1}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot [(\\vec{X} - \\vec{\\mu})^\\top \\Sigma^{-1}(\\vec{X} - \\vec{\\mu})]}$$\n",
    "\n",
    "```\n",
    "1.多元高斯分布:https://zhuanlan.zhihu.com/p/58987388\n",
    "  （高斯分布的推导）\n",
    "2.雅可比矩阵与雅可比行列式:https://zhuanlan.zhihu.com/p/39762178\n",
    "  (雅可比行列式在微积分换元中，给出了从x到y的n维体积的比率)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Class.Classification_2.png\" width=600 height=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"resource_pic/Class.Classification_3.png\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **拟合高斯分布：最大似然法**      \n",
    "该高斯分布用以估计$P(X|C_i)$,对于每一个类别$C_i$都存在一个高斯分布,输入是一个特征向量$x$,$\\mu$（均值）与$x$同维度,$\\Sigma$类似于一维时的方差\n",
    "$$L(\\mu,\\Sigma)=f_{\\mu,\\Sigma}(x^1)f_{\\mu,\\Sigma}(x^2)\\cdots f_{\\mu,\\Sigma}(x^n)$$\n",
    "$$\\Rightarrow \\mu^*=\\frac{1}{n}\\sum_{i=1}^n x^i\\; ,\\;\\Sigma^*=\\frac{1}{n}\\sum_{i=1}^n (x^i-\\mu^*)(x^i-\\mu^*)^T$$\n",
    "\n",
    "- **计算高斯分布下属于某类别的概率**     \n",
    "代入计算$P(C_1|x)=\\cfrac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2) }$         \n",
    "  二分类问题下，$P(C_i|x)>0.5$，则属于类别$i$\n",
    "\n",
    "\n",
    "- **改进方案**     \n",
    "  使用同一个$\\Sigma$     \n",
    "  (为什么采用同一个方差?Ref: Bishop, chapter 4.2.2)     \n",
    "  $$L(\\mu_1,\\mu_2,\\Sigma)=f_{\\mu_1,\\Sigma}(x^1)f_{\\mu_1,\\Sigma}(x^2)\\cdots f_{\\mu_1,\\Sigma}(x^n) f_{\\mu_2,\\Sigma}(x^{n+1})\\cdots f_{\\mu_2,\\Sigma}(x^{m})$$\n",
    "  1. $\\mu_1,\\mu_2\\;$is the same with former（即样本1与样本2的均值）     \n",
    "  2. $\\Sigma=\\Sigma_1\\frac{P(C_1)}{P(C_1)+P(C_2)}+\\Sigma_2\\frac{P(C_2)}{P(C_1)+P(C_2)}$\n",
    "\n",
    "\n",
    "- Probability Distribution     \n",
    "  - 选择合适的几率模型（比如：binary feature——伯努利分布）\n",
    "  - 对于高斯分布而言 如果一个变量的每个分量线性无关，那么她的高斯分布等于所有分量的高斯分布相乘,如果所有的维度都是无关的，那么是**朴素贝叶斯**    \n",
    "\n",
    "\n",
    "- **Posterior Probability(后验概率)**     \n",
    "  $$\\begin{align}\n",
    "   P(C_1|x)&=\\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+  P(x|C_2)P(C_2)}\\\\\n",
    "   &=\\frac{1}{1+\\cfrac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}\\\\\n",
    "   &=\\frac{1}{1+exp(-z)}\\\\\n",
    "   &=\\sigma(z)\n",
    "   \\end{align}$$\n",
    "   $$其中，z=ln(\\cfrac{{\\color{red}{P(x|C_1)P(C_1)}}}{P(x|C_2)P(C_2)})=ln(\\frac{P(x|C_1)}{P(x|C_2)})+ln(\\frac{P(C_1)}{P(C_2)})$$    \n",
    "   \n",
    "  将高斯分布的概率密度函数与$\\Sigma_1=\\Sigma_2=\\Sigma$代入后化简$\\;\\sigma(z)\\;$得到：      \n",
    "  $$z = (\\mu^1-\\mu^2)^T\\Sigma^{-1}x-\\frac{1}{2}(\\mu^1)^T\\Sigma^{-1}\\mu^1 +\\frac{1}{2}(\\mu^2)^T\\Sigma^{-1}\\mu^2+ln(\\frac{N_1}{N_2})$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 2. Logistic Regression     \n",
    "\n",
    "$$f_{w,b}=\\sigma(\\sum_i w_ix_i+b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个Training Data:($x^1,x^2,x^3\\cdots,x^N$和其相应的类别$C_1,C_2,C_1\\cdots,C_1$)      \n",
    "- **最大似然法**：     \n",
    "$$L(w,b)=f_{w,b}(x^1)(1-f_{w,b}(x^2))\\cdots f_{w,b}(x^N)$$     \n",
    "\n",
    "$$w^*,b^*=arg\\; \\underset{w,b}{max}L(w,b)$$\n",
    "\n",
    "$$\\Rightarrow w^*,b^*=arg\\; \\underset{w,b}{min}\\; -lnL(w,b)$$      \n",
    "其中，可以做如下转换：\n",
    "认为属于$C_1$的$\\hat y=1$,属于$C_2$的$\\hat y=0$    \n",
    "$$-lnf_{w,b}(x^1)=-[\\hat y^1lnf(x^1)+(1-\\hat y^1)ln(1-f(x^1)))$$\n",
    "\n",
    "$$\\Rightarrow -lnL(w,b)=-\\sum_n \\hat y^nlnf(x^n)+(1-\\hat y^n)ln(1-f(x^n)))$$       \n",
    "\n",
    "该式子实际上是两个伯努利分布的交叉熵公式    \n",
    "Distribution p:$p(x=1)=\\hat y^n\\; ,\\;p(x=0)=1-\\hat y^n$        \n",
    "Distribution q:$p(x=1)=\\hat f(x^n)\\; ,\\;p(x=0)=1-\\hat f(x^n)$      \n",
    "\n",
    "- **Cross entropy**: 代表的是两个分布的接近程度，交叉熵为0时两个分布相同\n",
    "  $$H(p,q)=-\\sum _x p(x)ln(q(x))$$\n",
    "  \n",
    "逻辑回归中只需要最小化Cross entropy(使用Square Error时，距离目标很远时，微分值依旧很小，会导致参数更新太快，并且容易出现错误)\n",
    "\n",
    "使用**Gradient Descent**最小化：$w_i=w_i-\\eta\\cfrac{-lnL(w,b)}{\\partial w_i}=w_i-\\eta\\sum _n-(\\hat y^n-f_{w,b}(x^n))x_i^n$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative（判别模型）与Generative（生成模型）    \n",
    "在判别模型中，可以通过$P(C_1|X)$直接通过梯度下降法计算$w,b$     \n",
    "在生成模型中，通过计算$\\mu^1,\\; \\mu^2 ,\\;\\Sigma ^{-1}$来求得参数     \n",
    "\n",
    "$$P(C_1|X)=\\sigma (w\\cdot x+b)$$\n",
    "两种模型在同一个Training data中得出的结果并不相同    \n",
    "在判别模型中，不作任何假设；在生成模型中，会做出例如高斯分布、伯努利分布、变量独立等的假设；     \n",
    "\n",
    "判别模型**通常**优于生成模型的原因：不加任何假设    \n",
    "- 生成模型的优势：     \n",
    "  1. 本身存在假设，需要的数据量较少    \n",
    "  2. 假设分布会忽略掉一些训练集中存在的噪声与错误（鲁棒）\n",
    "  3. Priors and class-dependent probabilities can be estimated from different sources.（先验和类相关概率可以从不同的来源进行估计。）      \n",
    "    在做判别模型时，先假设了后验概率       \n",
    "    在做生成模型时，假设了先验概率与类相关概率（**可以分开两个指标的数据来源**）      \n",
    "\n",
    "### 生成模型案例\n",
    "\n",
    "<img style=\"float: center;\" src=\"resource_pic/Class.Logistic%20Regression_1.png\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Classification\n",
    "- **softmax**     \n",
    "softmax用于多分类过程中，它将多个神经元的输出，映射到（0,1）区间内，看作概率，用以多分类      \n",
    "\n",
    "三分类：\n",
    "$$\\begin{align}\n",
    "C_1:\\;w^1,b_1\\;z_1=&w^1\\cdot x+b_1 \\\\\n",
    "C_2:\\;w^2,b_2\\;z_2=&w^2\\cdot x+b_3 \\\\\n",
    "C_3:\\;w^3,b_3\\;z_3=&w^3\\cdot x+b_3 \\\\\n",
    "y_1=&\\frac{e ^{z_1}}{\\sum _{j=1}^3 e^{z_j}}\\\\\n",
    "y_2=&\\frac{e ^{z_2}}{\\sum _{j=1}^3 e^{z_j}}\\\\\n",
    "y_3=&\\frac{e ^{z_3}}{\\sum _{j=1}^3 e^{z_j}}\n",
    "\\end{align}$$    \n",
    "\n",
    "即推广得：$$z=\\left [ \\begin{align}\n",
    "z_1\\\\\n",
    "z_2\\\\\n",
    "z_3\\\\\n",
    "\\vdots \\\\\n",
    "z_n\n",
    "\\end{align}\\right ]$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat y=\\left [ \\begin{align}\n",
    "y_1=&\\frac{e ^{z_1}}{\\sum _{j=1}^n e^{z_j}}\\\\\n",
    "y_2=&\\frac{e ^{z_2}}{\\sum _{j=1}^n e^{z_j}}\\\\\n",
    "y_3=&\\frac{e ^{z_3}}{\\sum _{j=1}^n e^{z_j}}\\\\\n",
    "&\\vdots \\\\\n",
    "y_n=&\\frac{e ^{z_n}}{\\sum _{j=1}^n e^{z_j}}\n",
    "\\end{align}\\right ]$$\n",
    "\n",
    "- cross entropy    \n",
    "Class 1:$\\hat y=\\left[ \\begin{aligned} 1\\\\ 0\\\\ \\vdots\\\\ 0\\end{aligned}\\right]$(转换为哑变量，其余以此类推)     \n",
    "Cross Entropy = $-\\sum_{i=1}^n \\hat y_i lny_i$     \n",
    "根据该回归输出的列变量中值最大（概率最大）的行所属类别即为所求类别"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitation of Logistic Regression         \n",
    "\n",
    "逻辑回归，实际上是选择一条直线将类别分为两类，但出现如下情况时便难以分类：     \n",
    "<img style=\"float: center;\" src=\"resource_pic/Class.Logistic%20Regression_2.png\" width=600 height=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解决方法：Feature Transformation**     \n",
    "  1. 例如，将$x_1,\\;x_2$转换为与$(0,0)$点的距离\n",
    "  2. 当我们需要机器进行自动转换时，可以选择通过逻辑回归自动进行特征的转换      \n",
    "  \n",
    "  将不同的维度（$x_1,\\;x_2$是维度而非实例），通过逻辑回归以一个权重重组\n",
    "  \n",
    "<img style=\"float: center;\" src=\"resource_pic/Class.Logistic%20Regression_3.png\" width=600 height=600>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
