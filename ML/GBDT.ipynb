{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858a9f0c",
   "metadata": {},
   "source": [
    "# CART\n",
    "CART树可以分为回归树和决策树。GBDT中的树是CART回归树。\n",
    "\n",
    "CART分类树算法使用**基尼系数**来代替信息增益比，**基尼系数代表了模型的不纯度**，基尼系数越小，不纯度越低，特征越好。\n",
    "\n",
    "在分类问题中，假设有$K$ 个类别，第$k$个类别的概率为$p_k$ ,则基尼系数为：\n",
    "\n",
    "$$Gini(P)=\\sum^K_{k=1}P_k(1-P_k) = 1 - \\sum^K_{k=1}P^2_k$$\n",
    "\n",
    "<img style=\"float: center;\" src=\"GBDT_pics/Cart_1.png\" width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98719b4",
   "metadata": {},
   "source": [
    "## 分类树\n",
    "\n",
    "1. **首先计算各特征的基尼指数，选择最优特征以及其最优切分点。特征记为$A_1,A_2,A_3,A_4$**\n",
    "\n",
    "以上述表格为例，计算年龄的基尼系数，首先年龄分为三个类型，需要计算**年龄特征$A_1$下三个类别的基尼系数**\n",
    "\n",
    "$$Gini(D,A_1 = 青年) =\\frac{5}{15} (2\\times \\frac{2}{5}\\times (1 - \\frac{2}{5})) + \\frac{10}{15} (2\\times \\frac{7}{10}\\times (1 - \\frac{7}{10}))$$\n",
    "\n",
    "$\\frac{5}{15}$是三个年龄类型中青年的概率，$\\frac{2}{5}$与$\\frac{7}{10}$是青年与非青年中正样本的比例\n",
    "\n",
    "2. **其次计算每个特征的最优切分点，找到基尼系数最小的特征切分点。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06db0c",
   "metadata": {},
   "source": [
    "## 回归树\n",
    "\n",
    "1. 原始数据集$S$，此时树的深度$depth=0$。\n",
    "\n",
    "2. 针对集合$S$，遍历每一个$feature$的每一个$value$，用该$value$将原数据集$S$分裂成2个集合：左集合$S_{left}$\n",
    " ($<=value$的样本)、右集合$S_{right}$($>value$的样本)，每一个集合也叫做一个结点。\n",
    " \n",
    "3. 分别计算这2个集合的$mse$，找到使得$(left_{mse}+right_{mse})$最小的那个$value$，记录下此时的$feature$名称和$value$，这个就是**最佳分割特征以及该特征的最佳分割值**；\n",
    "\n",
    "每一个集合/结点$mse$的计算方法如下:\n",
    "\n",
    "- $mean = \\frac{1}{N} \\sum_{i\\in S_{left}} \\ \\ y_i , mse = \\sum(y_i - mean)^2$，$mean$就是落在该叶子节点当中的预测值 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3505c",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。\n",
    "\n",
    "## Adaboost\n",
    "\n",
    "1. 确定样本集\n",
    "\n",
    "2. 初始化样本数据的权重，如每个样本的权重为1/n(假设样本个数为n)\n",
    "\n",
    "3. 进行1，2，...，T轮迭代\n",
    "\n",
    "        a. 归一化样本权重\n",
    "        b. 对样本集进行训练，并计算训练误差\n",
    "        c. 选择误差最小的分类器作为本轮的分类器\n",
    "        d. 根据预测结果更新样本数据的权重：预测错误样本增加权重，预测正确样本降低权重\n",
    "        e. 计算分类器的权重\n",
    "\n",
    "4. 根据分类器的预测结果及其权重加权表决最终结果\n",
    "\n",
    "### 例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd241027",
   "metadata": {},
   "source": [
    "|序号|1|2|3|4|5|6|7|8|9|10|\n",
    "|-|-|-|-|-|-|-|-|-|-|-|\n",
    "|x|0|1|2|3|4|5|6|7|8|9|\n",
    "|y|1|1|1|-1|-1|-1|1|1|1|-1|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa92735",
   "metadata": {},
   "source": [
    "- **第一个个体学习机**\n",
    "\n",
    "首先认为每个$x_i$的权重一样(本例中为$\\frac{1}{10}$)\n",
    "\n",
    "1. **计算切分点**\n",
    "\n",
    "第一个学习器的最优切分点为$2.5$，此时误差最小，即\n",
    "\n",
    "$$G_1(x) =  \\begin{cases}\n",
    "1, &x<2.5\\\\\n",
    "-1, &x>2.5\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "2.  **计算分类器权重**\n",
    "\n",
    "第一个分类器中有三个错误，即6，7，8\n",
    "\n",
    "计算误差$e_1 = 0.3$，计算分类器权重为$\\alpha_i = \\frac{1}{2}ln\\frac{1-e_i}{e_i}$(这么做的原因仅当$e_i<\\frac{1}{2},\\alpha_i>0$，学习器才有意义)\n",
    "\n",
    "3. **更新训练数据的权值分布**\n",
    "\n",
    "\n",
    "$w_{m+1,i} = \\cfrac{w_{m,i}}{Z_m}exp(-\\alpha_my_iG_m(x_i)), i = 1,2,...,N$\n",
    "\n",
    "$Z_m=w_{m,i}\\ exp(-\\alpha_my_iG_m(x_i))$\n",
    "\n",
    "其中$y_i,G_m(x_i)$为真实标签与预测标签，当预测正确时为$y_iG_m(x_i) = 1$，错误时$y_iG_m(x_i) = -1$，这样在公式中可以保证分类正确的权值下降，分类错误的上升\n",
    "\n",
    "其中$Z_m$为正则化参数，保证权值和唯一\n",
    "\n",
    "\n",
    "- **第m个个体学习机**\n",
    "\n",
    "第$m$个学习机在上一轮更新后的权值分布上重新重复计算过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53afa0a",
   "metadata": {},
   "source": [
    "# GBDT\n",
    "\n",
    "梯度提升决策树（Gradient Boosting Decision Tree，GBDT）是一种基于boosting集成学习思想的加法模型，训练时采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的负梯度**(平方损失函数中负梯度是残差)**。\n",
    "\n",
    "## GBDT回归树\n",
    "\n",
    "与CART类似的过程，拟合变为负梯度\n",
    "\n",
    "## GBDT分类树\n",
    "\n",
    "对数似然函数进行拟合多分类拟合"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
