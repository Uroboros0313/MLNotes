> # PRE
1. DQN
2. TD算法

> # DQN

- **目标**: maximize the total reward(获得最大化总回报)
- 如果已知$Q^*(s,a)$, 最优动作是:
    $$a^*=\underset{a}{argmax}\ Q^*(s,a)$$
    即根据当前的状态$s$, 在最优动作价值函数$Q^*$下, 最优动作$a^*$是什么
- 价值学习的基本思想: 学习出一个近似$Q^*$的函数
  - 解决方案: Deep Q Network(DQN)
  - 使用一个神经网络$Q(s,a;w)$去近似$Q^*(s,a)$
  - $w$是NN的参数, $s$是输入, $a$是输出

类比到Mario游戏当中, DRL首先对当前屏幕画面(state)通过DQN进行卷积以及全连接层, 最后输出**当前状态下采取每个动作的概率**。