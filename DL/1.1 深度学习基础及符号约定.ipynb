{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK1\n",
    "- 结构化数据：Data in databse（features-label）      \n",
    "- 非结构化数据：语音、图片、句子等\n",
    "\n",
    "传统机器学习算法在大量数据的表现上不够好，但大型神经网络在大型数据上的表现更优     \n",
    "<img style=\"float: center;\" src=\"course_1_pics/WEEK1_1.PNG\" width=600 height=600>    \n",
    "使用sigmoid函数的缺点：在某点的斜率很低，因此学习速度很慢,因此可能用ReLu函数替代     \n",
    "<img style=\"float: center;\" src=\"course_1_pics/WEEK1_2.PNG\" width=600 height=600>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK2    \n",
    "约定符号:   \n",
    "$$(x,y)-(features,label)\\\\\n",
    "x \\in R^{nx},代表nx维的向量,y\\in \\{0,1\\}\\\\\n",
    "(x^{(i)},y^{(i)})代表第i组样本\\\\\n",
    "m\\ training\\ example:{(x^{(1)},y^{(1)}),.....(x^{(m)},y^{(m)})},代表m个训练样本\\\\\n",
    "X=[x^{(1)},x^{(2)}...x^{(m)}]是训练集的输入值，放在一个nx\\times m的矩阵中\\\\\n",
    "Y=[y^{(1)},y^{(2)}...y^{(m)}]是训练集的输出值，放在一个1\\times m的矩阵中\\\\\n",
    "da代表对a的导数\\\\\n",
    "$$\n",
    "## Logistic Regression    \n",
    "$$Given\\ x,want\\ \\hat y = P{(y=1|x)} \\\\\n",
    "Parameters:w \\in R^{nx},b \\in R\\\\     \n",
    "Output, \\hat y =\\sigma(w^Tx+b),\\sigma (z)=\\frac{1}{1+e^{-z}}$$   \n",
    "\n",
    "**一种符号惯例**:设定一个参数$\\theta=[b,w]$,定义一个额外的变量 $x_0=1$，那么$x=[x_0,x_1,...x_{nx}]$，那么会得到$\\hat y =\\sigma(\\theta^Tx)$\n",
    "\n",
    "- **loss function损失函数**\n",
    "    - 选择损失函数$L(\\hat y,y)=\\frac{1}{2}(\\hat y-y)^2$，会让最后的优化问题成为非凸，存在多个局部最优解,梯度下降法会作用不大    \n",
    "    - 通常选择的损失函数:\n",
    "$$L(\\hat y,y)=-(ylog(\\hat y)+(1-y)log(1-\\hat y))$$     \n",
    "\n",
    "<font color='red' size = 6> 使用交叉熵函数的理由 </font>\n",
    "\n",
    "- **cost function代价函数**  \n",
    "为了衡量算法在**全部训练样本上**的表现如何，我们需要定义一个算法的代价函数，算法的代价函数是对个样本的损失函数求和然后除以$m$\n",
    "$$J(w,b) = \\frac{1}{m}\\sum_{i=1}^m L(\\hat y,y)$$\n",
    "损失函数只适用于训练样本，而代价函数是**参数的总代价**\n",
    "\n",
    "### Gradient Descent:   \n",
    "梯度下降法通过最小化代价函数（成本函数）来训练参数，\n",
    "<img style=\"float: center;\" src=\"course_1_pics/WEEK2_1.PNG\" width=600 height=600>       \n",
    "cost function是w和b的一个凸函数   \n",
    "\n",
    "**算法**：   \n",
    "\n",
    "$Repeat\\{\\\\\n",
    "w:=w-\\alpha \\frac{dJ(w,b)}{dw}\\\\\n",
    "b:=b-\\alpha \\frac{dJ(w,b)}{db}\\\\\n",
    "\\}\n",
    "$\n",
    "$\\alpha:Learning\\ rate(学习率)会更新$\n",
    "\n",
    "- **求导法则**：    \n",
    "$$\n",
    "J(w,b) = \\frac{1}{m}\\sum _{i=1}^m L(a^{(i)},y^{(i)}),\\frac{\\partial J}{\\partial w_1} =  \\frac{1}{m}\\sum _{i=1}^m w^{(i)}_1\\\\\n",
    "\\frac{dL(a,y)}{da} = \\frac{1-y}{a}-\\frac{y}{a}\\\\\n",
    "a = \\sigma(z),\\frac{da}{dz} = a(1-a)\\\\\n",
    "\\frac{dz}{dw_1} = x_1\\\\\n",
    "$$\n",
    "\n",
    "<font color='red' size = 6> 推导成本函数与损失函数间的关系以及回顾最大似然估计 </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK3      \n",
    "<img style=\"float: center;\" src=\"course_1_pics/WEEK3_1.PNG\" width=500 height=500> \n",
    "<img style=\"float: center;\" src=\"course_1_pics/WEEK3_2.PNG\" width=500 height=500> \n",
    "$a^{[i]}$代表第$i$层的输出值,$a^{[i]}_j$代表第$i$层第$j$个隐藏单元的输出值,$a^{[i](m)}$代表第$i$层神经网络在第$m$个样本下的输出      \n",
    "\n",
    "可供选择的激活函数:    \n",
    "- **tanh(z)**     \n",
    "几乎在所有场合都比Sigmoid更优越，但是二元分类或输出变量是0-1之间时可以选择使用sigmoid    \n",
    "存在**梯度消失**问题    \n",
    "$$tanh(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$$      \n",
    "$$tanh(z)' = 1-(tanh(z))^2$$\n",
    "- **Relu(z)/Leaky ReLu**——修正线性单元     \n",
    "如果输出值是0-1，可以输出选择Sigmoid，其他单元的激活函数全部使用ReLu      \n",
    "训练速度更快\n",
    "$$ReLu(z) = max(0,z)$$      \n",
    "$$Relu(z)' = \n",
    "\\begin{cases}\n",
    "1,&z>0\\\\\n",
    "0,&z<0\\\\\n",
    "undefined,&z=0\n",
    "\\end{cases}$$\n",
    "实际使用时z=0处定义为0或1 \n",
    "\n",
    "\n",
    "$$Leaky\\ ReLu(z) = max(0.01z,z)$$      \n",
    "$$Leaky\\ Relu(z)' = \n",
    "\\begin{cases}\n",
    "1,&z>0\\\\\n",
    "0.01,&z<0\\\\\n",
    "undefined,&z=0\n",
    "\\end{cases}$$\n",
    "实际使用时z=0处定义为0.01或1 \n",
    "### 为什么需要非线性激活函数？      \n",
    "需要通过神经网络拟合出多种函数。     \n",
    "如果去除非线性激活函数，那么输出只是输入的线性组合。   \n",
    "\n",
    "### 为什么需要随机初始化参数\n",
    "<img style=\"float: center;\" src=\"course_1_pics/WEEK3_2.PNG\" width=500 height=500>    \n",
    "假设同一层$w^{[1]} = \n",
    "\\begin{bmatrix}\n",
    "0& 0      \\\\\n",
    "0& 0\n",
    "\\end{bmatrix}$\n",
    "由于同层神经元当中的激活函数对称，因此在反向传播时两个神经元计算的导数相同，计算的是同一个函数，因此多个神经元没有意义。     \n",
    "\n",
    "通常在随机初始化参数时，通常把权重初始化为一个非常小的值，如果z值过大或者过小，会导致Sigmoid函数或tanh函数出现梯度消失/减慢学习速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK4     \n",
    "- **为什么DEEP？**     \n",
    "从电路理论理解——深层的结构比横向的结构能够表示更复杂的函数，需要的参数更少      \n",
    "\n",
    "Hyper parameter(超参数):控制参数的参数     \n",
    "- 学习率   \n",
    "- 迭代次数    \n",
    "- 隐层数     \n",
    "- 隐层当中的神经元数   \n",
    "- 激活函数的选择    \n",
    "- 正则化参数    \n",
    "- mini-batch size     \n",
    "\n",
    "等等\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
