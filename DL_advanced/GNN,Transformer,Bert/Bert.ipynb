{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1071d967",
   "metadata": {},
   "source": [
    "# Self-supervised learning\n",
    "\n",
    "- **supervised**       \n",
    "    需要有feature，有label\n",
    "    \n",
    "    \n",
    "- **self-supervised**     \n",
    "    假设有无标注的资料，则将资料的一部分作为***label***($x''$),一部分是***feature***($x''$)            \n",
    "    将$x'$输入模型当中，模型计算出$y$，令$y$尽量与$x''$相等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf68043",
   "metadata": {},
   "source": [
    "# Bert\n",
    "\n",
    "## Masking Input\n",
    "\n",
    "***Bert***是***Transformer***的编码器的架构——输入一个序列，输出一个序列\n",
    "\n",
    "1. 输入序列时，随机产生掩码遮住一部分***token(Randomly masking some token)***\n",
    "\n",
    "- 具体做法:\n",
    "    - 掩码盖住的部分加上***mask***修改为***special token***\n",
    "    - 掩码盖住的部分修改为随机的向量(随机的字符等)\n",
    "    \n",
    "2. ***Bert***输出一列向量，最后将掩码遮盖的部分的输出做一个***Linear Transform***(乘以一个矩阵)，最后经过***softmax***输出一个***vocabulary的distribution***\n",
    "\n",
    "\n",
    "3. 训练目标——掩码遮盖的部分的交叉熵最小(遮盖住的部分就是**标签**)\n",
    "\n",
    "\n",
    "<img style=\"float: center;\" src=\"resource_pic/Bert/Bert_1.PNG\" width=450 height=450>\n",
    "\n",
    "\n",
    "## Next Sentence Prediction(useless)     \n",
    "\n",
    "1. 在数据中选取两个句子，在两个句子中间插入一个特殊符号`[SEP]`作为分隔符，并在第一个句子前加入符号`[CLS]`组成一个序列\n",
    "\n",
    "\n",
    "2. 将该序列输入***Bert***，并输出一个序列，此时仅关注`[CLS]`的输出，将该输出经过一个***Linear Transform***后进行二分类(1/0)，该二分类问题判断两个句子是否相连接\n",
    "\n",
    "\n",
    "<img style=\"float: center;\" src=\"resource_pic/Bert/Bert_2.PNG\" width=450 height=450>\n",
    "\n",
    "### 其余方法\n",
    "\n",
    "由于***Next Sentence Prediction***任务可能较为简单，因此有其他方法    \n",
    "\n",
    "- ***SOP(Sentence order prediction)***        \n",
    "将两个连接的句子输入后，判断句子的顺序\n",
    "\n",
    "## Bert的用处    \n",
    "\n",
    "***Bert***通常被用在下游的任务中(***DownStream Tasks***),即实际上关注的任务，该模型可以通过***Fine-tune***(把***Bert***做微调)\n",
    "\n",
    "\n",
    "生成***Bert***的过程——***Pre-train***\n",
    "\n",
    "## GLUE(General Language Understanding Evaluation)       \n",
    "\n",
    "一个***NLP***的任务集，一共有九个任务，***Bert***模型在九个任务上进行***Fine-tune***测试性能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ceec15",
   "metadata": {},
   "source": [
    "# How to use Bert\n",
    "\n",
    "## Case 1\n",
    "```\n",
    "Input:sequence\n",
    "output:class\n",
    "Example:sentiment analysis\n",
    "\n",
    "It is good——>positive\n",
    "```\n",
    "\n",
    "1. 将`[CLS]`的***token***与句子输入***Bert***，`[CLS]`对应的输出经过一个***Linear Transform***后进行分类(此时需要有下游任务的标注数据)\n",
    "\n",
    "\n",
    "2. ***Linear Transform***进行随机初始化\n",
    "\n",
    "\n",
    "3. ***Bert***的初始化采用***pre-train***的参数    \n",
    "\n",
    "<img style=\"float: center;\" src=\"resource_pic/Bert/Bert_3.PNG\" width=450 height=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f462b",
   "metadata": {},
   "source": [
    "## Case 2\n",
    "```\n",
    "Input:sequence\n",
    "output:same length as input\n",
    "Example:POS tagging\n",
    "\n",
    "I——>N\n",
    "saw——>V\n",
    "a——>DET\n",
    "saw——>N\n",
    "```\n",
    "\n",
    "将`[CLS]`的***token***与句子输入***Bert***，句子中的每个***token***对应输出经过一个***Linear Transform***后进行***softmax***分类\n",
    "\n",
    "<img style=\"float: center;\" src=\"resource_pic/Bert/Bert_4.PNG\" width=300 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8eab4d",
   "metadata": {},
   "source": [
    "## Case 3\n",
    "```\n",
    "Input:two sequence\n",
    "output:a class\n",
    "Example:NLI\n",
    "\n",
    "(前提)premise: A person on a horse jumps over a broken down airplane\n",
    "(假设)hypothesis: A person is at a diner\n",
    "```\n",
    "- **NLI(Natural Language Inference)**,自然语言推理。它主要用来判断两个句子在语义上的关系，一般可以分为：***Entailment(蕴含),Contradiction(矛盾),Neutral(中立)***\n",
    "\n",
    "\n",
    "<img style=\"float: center;\" src=\"resource_pic/Bert/Bert_5.PNG\" width=300 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb93e3a9",
   "metadata": {},
   "source": [
    "## Case 3\n",
    "`Input :`\n",
    "\n",
    "$Document:D = {d_1,d_2,...d_N}$,每个$d_i$代表一个文章中的词/字\n",
    "\n",
    "$Query:Q = {q_1,q_2,...q_N}$,每个$q_i$代表一个问题中的词/字\n",
    "\n",
    "`Output:`\n",
    "\n",
    "将$D,Q$输入模型中,输出两个正整数$s,e$,代表文章中的答案的序号区间,则答案是$A={d_s,...,d_e}$\n",
    "\n",
    "\n",
    "1. 首先初始化两个矩阵(橙色与蓝色部分),分别是答案开始位置和答案结束位置的$Query$,这两个矩阵需要学习。\n",
    "\n",
    "\n",
    "2. 将两个矩阵分别乘以不同的词/字通过***Bert***产生的$Key$,即***Bert***的输出，输出长度等于输入的文章长度。\n",
    "\n",
    "\n",
    "3. 将$Query$与$Key$经过***inner product***产生的向量输入***softmax***,输出一个答案开始与结束序号概率值分布。\n",
    "\n",
    "结构如下图:\n",
    "\n",
    "<img style=\"float: center;\" src=\"resource_pic/Bert/Bert_6.PNG\" width=300 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d3ea2",
   "metadata": {},
   "source": [
    "# Why does Bert work?\n",
    "\n",
    "***Bert***输出的是一个词的***Embedding***,并且考虑了上下文,一词多义的情况会被避免         \n",
    "\n",
    "***Bert***通过遮盖一个词，经过**上下文的训练**得到一个词语的词义(Contextualized word embedding)\n",
    "\n",
    "## Multi-lingual BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c28a31",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
