{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Slot Filling**       \n",
    "\n",
    "**case 1:** ticket booking system    \n",
    "客户对系统：i would like to arrive Taipei on November 2nd      \n",
    "$$Slot\n",
    "\\begin{cases} \n",
    "Destination  & Taipei\\\\\n",
    "time of arrival & November 2nd\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "把Taipei这样词汇转化为一个vector，输入到神经网络中    \n",
    "<img style=\"float: center;\" src=\"resource_pic/Class7.RNN_1.PNG\" width=600 height=600>\n",
    "- 1 of N 编码（One-hot）   \n",
    "  ```\n",
    "  apple = [1,0,0,...,0]   \n",
    "  bag = [0,1,0,...,0]    \n",
    "  trouser = [0,0,1,...,0]      \n",
    "  ```\n",
    "  可以另外加一个属性$x_{n+1}$代表不在vector内的词汇other     \n",
    "  ```\n",
    "  other(不在词汇空间内) = [0,0,0...,0,1]\n",
    "  ```\n",
    "\n",
    "\n",
    "- Word hash     \n",
    "  26×26×26的字母空间  \n",
    "  ```\n",
    "  apple在[a,p,p],[p,p,l],[p,l,e]处的值为1\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**case2：**   \n",
    "当有两个输入Taipei分别是目的地与出发地,但同一个输入而言，输出应该是一样的。     \n",
    "因此如图，希望神经元是有记忆的，输入Taipei之前能够记忆它的前一个动词arrive/leave     \n",
    "\n",
    "<img style=\"float: center;\" src=\"resource_pic/Class7.RNN_2.PNG\" width=600 height=600>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN    \n",
    "```   \n",
    "假设一个神经网络weights都是1，没有bias\n",
    "1. 给memory模块一个初值[0,0]    \n",
    "2. 输入[1 1],绿色Neuron变为[2,2]     \n",
    "3. 红色Neuron输出[4,4]    \n",
    "4. 更新memory为[2,2]    \n",
    "5. 输入[1,1],绿色输出[6,6],红色输出[12,12],更新蓝色[6,6]     \n",
    "```    \n",
    "### **如果交换输入顺序，那么RNN输出结果完全不同，即依赖于输入顺序**\n",
    "\n",
    "<img style=\"float: left;\" src=\"resource_pic/Class7.RNN_3.PNG\" width=400 height=400>\n",
    "<img style=\"float: right;\" src=\"resource_pic/Class7.RNN_4.PNG\" width=400 height=400>      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Elman/Jordan/Biderectional**              \n",
    "1. Elman Network：Memory通过hidden layer储存，再返回给hidden layer     \n",
    "2. Jordan Network：Memory通过输出层储存，由于有一个target（目标值），再返回给hidden layer表现更好     \n",
    "3. Bidirectional RNN：从正反两个方向读取input，把这两个方向的hidden layer接给同一个output layer得到一个输出y     \n",
    "\n",
    "- **Long Short-term Memory(LSTM)**       \n",
    "LSTM的Memory cell有三个gate：\n",
    "1. 写入memory通过input gate         \n",
    "2. 输出memory通过output gate      \n",
    "3. memmory的forget由Forget gate控制     \n",
    "\n",
    "input/output/forget gate打开或者关起来由神经网络自行学习\n",
    "\n",
    "<img style=\"float: left;\" src=\"resource_pic/Class7.RNN_5.PNG\" width=490 height=490>\n",
    "<img style=\"float: left;\" src=\"resource_pic/Class7.RNN_6.PNG\" width=490 height=490>\n",
    "\n",
    "## LSTM的逻辑：\n",
    "\n",
    "1. 设memory cell的当前值（初值）为$c$，输入一个$z$  \n",
    "\n",
    "\n",
    "2. 通过一个输入函数$g$得到$g(z)$\n",
    "\n",
    "\n",
    "3. input gate输入一个$z_i$，经过激活函数f得到一个$f(z_i)$,得到memory cell的输入值$g(z)f(z_i)$（激活函数通常使用sigmoid function，值域$[0,1]$，可以模仿一个门的开闭） \n",
    "\n",
    "\n",
    "4. forget gate输入一个$z_f$，经过激活函数f得到一个$f(z_f)$，得到memory cell的遗忘值$cf(z_f)$  \n",
    "\n",
    "\n",
    "5. 更新memory cell的值$c \\Rightarrow c'=g(z)f(z_i)+cf(z_f)$        \n",
    "\n",
    "\n",
    "6. $c'$经过一个output function得到$h(c')$，input gate输入一个$z_o$，经过激活函数f得到一个$f(z_o)$，相乘得到LSTM神经元的输出$a=h(c')f(z_o)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM和RNN的关系\n",
    "- **LSTM有四个input和一个output，即三个gate和一个输入值与一个输出值——LSTM就是把普通的neuron换成LSTM神经元**  \n",
    "\n",
    "<img style=\"float: center;\" src=\"resource_pic/Class7.RNN_7.PNG\" width=490 height=490>\n",
    "\n",
    "   在并列的每个LSTM的memory cell都储存了一个scaler，把所有的scaler连接起来，即形成一个vector——$c^{t-1}$      \n",
    "   \n",
    "   在某个时间点input一个vector——$x^t$,乘一个矩阵变成另外一个vector——$z$，$z$的每一个dimension代表操控每个LSTM的**input**，$z$的dimension正好是LSTM的memory cell的个数\n",
    "\n",
    "   同样的vector——$x^t$,乘另外一个矩阵变成另外一个vector——$z^i$，$z^i$的每一个dimension代表操控每个LSTM的**input gate**，$z$的dimension正好是LSTM的memory cell的个数    \n",
    "\n",
    "同理，$z_f$和$z_o$，对应另外两个gate\n",
    "\n",
    "真正的LSTM会把output和memory同时拿出来作为修正下一个时刻的输入\n",
    "\n",
    "<img style=\"float: center;\" src=\"resource_pic/Class7.RNN_8.PNG\" width=490 height=490>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 减少ERROR的方法     \n",
    "原因：对于参数而言，ERROR随着参数的变化非常陡峭    \n",
    "# 截图8     \n",
    "Clipping：gradient大于某个值时，停止training。      \n",
    "（通常不认为是激活函数的方法）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设一个最简单的RNN，输入[1,0,0...,0]，由于这是一个很长的序列，w一旦有变化，由于记忆性w的一点变化在反复重复使用以后，就会发生很大的影响    \n",
    "解决方法——LSTM\n",
    "LSTM可以做到可以处理梯度消失的问题\n",
    "RNN中，每输入一个新的数据memory会被重置 但LSTM里Memory是相加的，Memory不会把过去的内容清除\n",
    "# 截图9\n",
    "GRU      \n",
    "联动input gate 和forget gate      \n",
    "打开input gate打开就会关闭forget gate，就会format掉memory cell的值。当forget gate不清理memory cell时，input无法输入值         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
