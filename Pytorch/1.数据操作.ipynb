{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据操作\n",
    "\n",
    "## 创建数组\n",
    "在PyTorch中，torch.Tensor是存储和变换数据的主要工具。Tensor提供GPU计算和自动求梯度等更多功能。\n",
    "\n",
    "\"tensor\"这个单词一般可译作“张量”，张量可以看作是一个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:49:46.117363Z",
     "start_time": "2022-01-28T08:49:46.109349Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:49:46.381620Z",
     "start_time": "2022-01-28T08:49:46.357686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no init:\n",
      " tensor([[8.9082e-39, 6.9796e-39, 9.0919e-39],\n",
      "        [9.9184e-39, 7.7143e-39, 1.0010e-38],\n",
      "        [8.4490e-39, 1.0286e-38, 9.8266e-39],\n",
      "        [1.0469e-38, 9.2755e-39, 8.7245e-39],\n",
      "        [5.2347e-39, 5.1429e-39, 4.6837e-39]])\n",
      "\n",
      " random init:\n",
      " tensor([[0.6278, 0.8419, 0.8850],\n",
      "        [0.4946, 0.4858, 0.6241],\n",
      "        [0.7569, 0.6837, 0.5785],\n",
      "        [0.8162, 0.0562, 0.7390],\n",
      "        [0.5574, 0.8088, 0.3911]])\n",
      "\n",
      " zero init with long type:\n",
      " tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "\n",
      " create tensor by values:\n",
      " tensor([5.5000, 3.0000])\n",
      "\n",
      " ones init:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "\n",
      " create by existing tensor:\n",
      " tensor([[ 0.0753, -0.7372, -0.8509],\n",
      "        [-0.5709, -0.7559,  1.0408],\n",
      "        [-0.2600,  0.5239, -1.6627],\n",
      "        [-0.8166, -1.0407,  1.1757],\n",
      "        [ 1.0029,  0.3438, -0.1568]])\n"
     ]
    }
   ],
   "source": [
    "# 创建未初始化的tensor\n",
    "x = torch.empty(5,3)\n",
    "print(\"no init:\\n\",x)\n",
    "\n",
    "# 创建随机初始化的tensor\n",
    "x = torch.rand(5,3)\n",
    "print(\"\\n random init:\\n\",x)\n",
    "\n",
    "#long型全0数组\n",
    "x = torch.zeros(5,3,dtype = torch.long)\n",
    "print(\"\\n zero init with long type:\\n\",x)\n",
    "\n",
    "#直接根据数据创建\n",
    "x = torch.tensor([5.5,3])\n",
    "print(\"\\n create tensor by values:\\n\",x)\n",
    "\n",
    "#通过现有的tensor创建，会默认重用输入tensor的一些属性\n",
    "#全1矩阵\n",
    "x = x.new_ones(5,3,dtype = torch.float64)#返回的tensor有相同的torch.dtype和torch.device\n",
    "print(\"\\n ones init:\\n\",x)\n",
    "\n",
    "#随机矩阵\n",
    "x = torch.randn_like(x,dtype = torch.float)#指定数据类型\n",
    "print(\"\\n create by existing tensor:\\n\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:49:48.081594Z",
     "start_time": "2022-01-28T08:49:48.073848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|函数|功能|\n",
    "|:-:|:-:|\n",
    "|Tensor(*sizes)\t|基础构造函数|\n",
    "|tensor(data,)\t|类似np.array的构造函数|\n",
    "|ones(*sizes)\t|全1Tensor|\n",
    "|zeros(*sizes)\t|全0Tensor|\n",
    "|eye(*sizes)\t|对角线为1，其他为0|\n",
    "|arange(s,e,step)\t|从s到e，步长为step|\n",
    "|linspace(s,e,steps)\t|从s到e，均匀切分成steps份|\n",
    "|rand/randn(*sizes)\t|均匀/标准分布|\n",
    "|normal(mean,std)/uniform(from,to)\t|正态分布/均匀分布|\n",
    "|randperm(m)\t|随机排列$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 操作 \n",
    "\n",
    "### 算数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:52:26.589841Z",
     "start_time": "2022-01-28T08:52:26.580865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x+y:\n",
      " tensor([[ 1.0608, -0.1471, -0.3967],\n",
      "        [ 0.3624,  0.0083,  1.9584],\n",
      "        [ 0.0354,  0.9472, -1.1002],\n",
      "        [-0.6092, -0.6832,  1.4037],\n",
      "        [ 1.3033,  1.1648,  0.7210]])\n",
      "\n",
      " torch.add(x,y):\n",
      " tensor([[ 1.0608, -0.1471, -0.3967],\n",
      "        [ 0.3624,  0.0083,  1.9584],\n",
      "        [ 0.0354,  0.9472, -1.1002],\n",
      "        [-0.6092, -0.6832,  1.4037],\n",
      "        [ 1.3033,  1.1648,  0.7210]])\n",
      "\n",
      "指定输出:\n",
      " tensor([[ 1.0608, -0.1471, -0.3967],\n",
      "        [ 0.3624,  0.0083,  1.9584],\n",
      "        [ 0.0354,  0.9472, -1.1002],\n",
      "        [-0.6092, -0.6832,  1.4037],\n",
      "        [ 1.3033,  1.1648,  0.7210]])\n",
      "\n",
      " inplace加法,y.add_(x):\n",
      " tensor([[ 1.0608, -0.1471, -0.3967],\n",
      "        [ 0.3624,  0.0083,  1.9584],\n",
      "        [ 0.0354,  0.9472, -1.1002],\n",
      "        [-0.6092, -0.6832,  1.4037],\n",
      "        [ 1.3033,  1.1648,  0.7210]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "print(\"x+y:\\n\",x+y)\n",
    "\n",
    "print(\"\\n torch.add(x,y):\\n\",torch.add(x,y))\n",
    "\n",
    "result = torch.empty(5,3)\n",
    "torch.add(x,y,out = result)#指定输出的数组\n",
    "print(\"\\n指定输出:\\n\",result)\n",
    "\n",
    "y.add_(x)\n",
    "print(\"\\n inplace加法,y.add_(x):\\n\",y)\n",
    "#pytorch操作inplace版本都有后缀_,例如x.copy(y),x.t_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引\n",
    "类似NumPy的索引操作来访问Tensor的一部分，需要注意的是：**索引出来的结果与原数据共享内存**，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:29.510462Z",
     "start_time": "2022-01-28T08:48:29.495503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5199, 0.5101, 0.5206])\n",
      "tensor([1.5199, 0.5101, 0.5206])\n"
     ]
    }
   ],
   "source": [
    "y = x[0,:]\n",
    "y+=1\n",
    "print(y)\n",
    "print(x[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|函数|\t功能|\n",
    "|:-:|:-:|\n",
    "|index_select(input, dim, index)\t|在指定维度dim上选取，比如选取某些行、某些列|\n",
    "|masked_select(input, mask)\t|例子如上，a[a>0]，使用ByteTensor进行选取|\n",
    "|nonzero(input)|\t非0元素的下标|\n",
    "|gather(input, dim, index)|\t根据index，在dim维度上选取数据，输出的size与index一样|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:29.526420Z",
     "start_time": "2022-01-28T08:48:29.512457Z"
    }
   },
   "outputs": [],
   "source": [
    "a= torch.randn(4,3)\n",
    "# 用法1\n",
    "# 第一种用法 c会成为一个新的张量且不会和a共用内存\n",
    "c = torch.index_select(a,1,torch.tensor([0]))\n",
    "# 用法2\n",
    "# 第二种用法d会和a共用内存 \n",
    "d = a.index_select(1,torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:29.542416Z",
     "start_time": "2022-01-28T08:48:29.528414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3144, -0.7410,  0.4588],\n",
       "         [-0.6570,  0.6372, -1.4192],\n",
       "         [-0.1012, -0.4568, -2.0608]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,3,3).index_select(0,torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:29.558369Z",
     "start_time": "2022-01-28T08:48:29.543374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2130, 1.1274, 0.5048, 0.9305, 0.1160, 2.0087])\n",
      "tensor([0.2130, 1.1274, 0.5048, 0.9305, 0.1160, 2.0087])\n",
      "tensor([0.2130, 1.1274, 0.5048, 0.9305, 0.1160, 2.0087])\n"
     ]
    }
   ],
   "source": [
    "print(a.masked_select(a>0))\n",
    "print(torch.masked_select(a,a>0))\n",
    "print(a[a>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:29.574291Z",
     "start_time": "2022-01-28T08:48:29.560328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(4,4)\n",
    "a[1,2]+=1\n",
    "torch.nonzero(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T09:00:04.198883Z",
     "start_time": "2022-01-28T09:00:04.183924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6688],\n",
      "        [0.6045],\n",
      "        [0.6964]])\n",
      "tensor([[0.6688, 0.2973],\n",
      "        [0.6045, 0.6045],\n",
      "        [0.6964, 0.2374]])\n"
     ]
    }
   ],
   "source": [
    "# dim=1表示,\n",
    "# 从input中的每一行中,选择index=[[1],[0],[0]],即选择每一行的第1,0,0个元素,构成新tensor\n",
    "\n",
    "input = torch.tensor([[0.2973, 0.6688],\n",
    "        [0.6045, 0.5933],\n",
    "        [0.6964, 0.2374]])\n",
    " \n",
    "index = torch.tensor([[1],\n",
    "        [0],\n",
    "        [0]])\n",
    "\n",
    "print(torch.gather(input,dim=1,index=index))\n",
    "\n",
    "#dim=1/0表示,从input中的每一行/列中,选择index=[[1,0],[0,0],[0,1]],即选择第一行的第1,0个,第二行选择第0,0个,第三行选择第0,1个元素,构成新tensor\n",
    "index = torch.tensor([[1, 0],\n",
    "        [0, 0],\n",
    "        [0, 1]])\n",
    "                            \n",
    "print(torch.gather(input,dim=1,index=index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改变形状\n",
    "\n",
    "注意view()返回的新Tensor与源Tensor虽然可能有不同的size，但是是**共享data的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T09:07:09.115206Z",
     "start_time": "2022-01-28T09:07:09.108160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) \n",
      " torch.Size([15]) \n",
      " torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "y = x.view(15)\n",
    "z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\n",
    "print(x.size(),\"\\n\", y.size(),'\\n', z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:29.622164Z",
     "start_time": "2022-01-28T08:48:29.607203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5199, -0.4899, -0.4794],\n",
      "        [-1.1256,  0.1002,  0.6374],\n",
      "        [-1.2400, -1.3876, -1.6877],\n",
      "        [-1.4366,  0.3284,  0.5157],\n",
      "        [-2.1807,  0.7099, -0.7837]])\n",
      "tensor([ 1.5199,  0.5101,  0.5206, -0.1256,  1.1002,  1.6374, -0.2400, -0.3876,\n",
      "        -0.6877, -0.4366,  1.3284,  1.5157, -1.1807,  1.7099,  0.2163])\n"
     ]
    }
   ],
   "source": [
    "# 返回一个真正新的副本（即不共享data内存）\n",
    "# 使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor\n",
    "x_cp = x.clone().view(15)\n",
    "x -= 1\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:29.638121Z",
     "start_time": "2022-01-28T08:48:29.624158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9740490317344666\n"
     ]
    }
   ],
   "source": [
    "# 转换为python数字\n",
    "print(torch.randn(1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性代数\n",
    "|函数\t|功能|\n",
    "|:-:|:-:|\n",
    "|trace|\t对角线元素之和(矩阵的迹)|\n",
    "|diag\t|对角线元素|\n",
    "|triu/tril\t|矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm\t|矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/baddbmm|\t矩阵运算|\n",
    "|t\t|转置|\n",
    "|dot/cross|\t内积/外积|\n",
    "|inverse|\t求逆矩阵|\n",
    "|svd\t|奇异值分解|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:29.654078Z",
     "start_time": "2022-01-28T08:48:29.639118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运算的内存开销     \n",
    "\n",
    "索引操作是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存\n",
    "\n",
    "**注**：虽然view返回的Tensor与源Tensor是共享data的，但是依然是一个新的Tensor（因为Tensor除了包含data外还有一些其他属性），**二者id（内存地址）并不一致**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:29.670035Z",
     "start_time": "2022-01-28T08:48:29.655075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2509139775800 2509139779320 False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2])\n",
    "y = torch.tensor([3,4])\n",
    "# Python自带的id函数\n",
    "id_before = id(y)\n",
    "y = y+x\n",
    "print(id(y),id_before,id(y)==id_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:29.685993Z",
     "start_time": "2022-01-28T08:48:29.671033Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2509139817880 2509139817880 True\n",
      "2509139818840 2509139818840 True\n"
     ]
    }
   ],
   "source": [
    "# 想指定结果到原来的y的内存，可以使用索引来进行替换操作\n",
    "x_1 = torch.tensor([1,2])\n",
    "y_1 = torch.tensor([3,4])\n",
    "\n",
    "id_before = id(y_1)\n",
    "\n",
    "y_1[:] = y_1+x_1\n",
    "\n",
    "print(id(y_1),id_before,id(y_1)==id_before)\n",
    "\n",
    "# 还可以使用运算符全名函数中的out参数/自加运算符+=(也即add_())达到上述效果\n",
    "x_2 = torch.tensor([1,2])\n",
    "y_2 = torch.tensor([3,4])\n",
    "\n",
    "id_before = id(y_2)\n",
    "\n",
    "torch.add(x_2,y_2,out = y_2)\n",
    "# y+=x\n",
    "# y.add_(x)\n",
    "print(id(y_2),id_before,id(y_2)==id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tensor和Numpy互相转换           \n",
    "用`numpy()`和`from_numpy()`将Tensor和NumPy中的数组**相互转换**。但是需要注意的一点是： 这两个函数所产生的的Tensor和NumPy中的数组**共享相同的内存**\n",
    "\n",
    "还有一个常用的将NumPy中的array转换成Tensor的方法就是`torch.tensor()`, 需要注意的是，此方法总是会进行**数据拷贝**（就会消耗更多的时间和空间），所以返回的Tensor和原来的数据**不共享内存**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:29.701950Z",
     "start_time": "2022-01-28T08:48:29.687988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n",
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)\n",
    "\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:29.717909Z",
     "start_time": "2022-01-28T08:48:29.702947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#所有在CPU上的Tensor（除了CharTensor）都支持与NumPy数组相互转换。\n",
    "# 用torch.tensor()将NumPy数组转换成Tensor，需要注意的是该方法总是会进行数据拷贝，返回的Tensor和原来的数据不再共享内存。\n",
    "\n",
    "c = torch.tensor(a)\n",
    "a += 1\n",
    "print(a, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T09:54:24.230077Z",
     "start_time": "2022-01-28T09:54:24.178217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0753, -0.7372, -0.8509],\n",
      "        [-0.5709, -0.7559,  1.0408],\n",
      "        [-0.2600,  0.5239, -1.6627],\n",
      "        [-0.8166, -1.0407,  1.1757],\n",
      "        [ 1.0029,  0.3438, -0.1568]])\n",
      "tensor([[ 1.0753,  0.2628,  0.1491],\n",
      "        [ 0.4291,  0.2441,  2.0408],\n",
      "        [ 0.7400,  1.5239, -0.6627],\n",
      "        [ 0.1834, -0.0407,  2.1757],\n",
      "        [ 2.0029,  1.3438,  0.8432]], device='cuda:0')\n",
      "tensor([[ 1.0753,  0.2628,  0.1491],\n",
      "        [ 0.4291,  0.2441,  2.0408],\n",
      "        [ 0.7400,  1.5239, -0.6627],\n",
      "        [ 0.1834, -0.0407,  2.1757],\n",
      "        [ 2.0029,  1.3438,  0.8432]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 用方法to()可以将Tensor在CPU和GPU（需要硬件支持）之间相互移动。\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    print(x)\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动梯度       \n",
    "PyTorch提供的autograd包能够根据输入和前向传播过程自动构建计算图，并执行反向传播    \n",
    "\n",
    "## 概念   \n",
    "\n",
    "1. **Tensor是这个包的核心类**     \n",
    "    如果将其属性`.requires_grad`设置为`True`，它将开始追踪(track)在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用`.backward()`来完成所有梯度计算。**此Tensor的梯度将累积到`.grad`属性中**。                \n",
    "    - `.detach()`:如果不想要被继续追踪，可以调用`.detach()`其将其从追踪记录中分离出来。         \n",
    "    \n",
    "    - `with torch.no_grad()`：将不想被追踪的操作代码块包裹起来。在评估模型的时候很常用，因为在评估模型时，并不需要计算可训练参数（`requires_grad=True`）的梯度。\n",
    "    \n",
    "**注意在y.backward()时，如果y是标量**，则不需要为backward()传入任何参数；否则，需要传入一个与y同形的Tensor。\n",
    "\n",
    "\n",
    "\n",
    "2. **Function是另外一个很重要的类**。       \n",
    "\n",
    "    Tensor和Function互相结合就可以构建一个**记录有整个计算过程的有向无环图**（`DAG`）。             \n",
    "\n",
    "    **每个Tensor都有一个`.grad_fn`属性**，该属性即创建该Tensor的Function, 就是说该Tensor是不是通过某些运算得到的，若是，则`grad_fn`返回一个与这些运算相关的对象，否则是None。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tensor\n",
    "创建一个Tensor并设置requires_grad=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:33.824145Z",
     "start_time": "2022-01-28T08:48:33.811179Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor x: tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "grad func of tensor x: None\n",
      "\n",
      " tensor y: tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "grad func of tensor y: <AddBackward0 object at 0x00000248345F1F88>\n",
      "\n",
      " True False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad = True)   \n",
    "print(\"tensor x:\",x)\n",
    "print(\"grad func of tensor x:\",x.grad_fn)\n",
    "\n",
    "y = x+2\n",
    "print(\"\\n tensor y:\",y)\n",
    "print(\"grad func of tensor y:\",y.grad_fn)   \n",
    "\n",
    "print(\"\\n\",x.is_leaf, y.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意x是直接创建的，所以它没有grad_fn, 而y是通过一个加法操作创建的，所以它有一个为`<AddBackward>`的grad_fn。\n",
    "\n",
    "像**x这种直接创建的称为叶子节点，叶子节点对应的grad_fn是None**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:33.839106Z",
     "start_time": "2022-01-28T08:48:33.826141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) \n",
      " tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y*3 \n",
    "out = z.mean()\n",
    "print(z,\"\\n\",out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过**.requires_grad_()**来用in-place的方式改变requires_grad属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:33.855062Z",
     "start_time": "2022-01-28T08:48:33.841100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000024834632F48>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # False\n",
    "\n",
    "a.requires_grad_(True)\n",
    "\n",
    "print(a.requires_grad) # True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:33.871020Z",
     "start_time": "2022-01-28T08:48:33.857057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# 因为out是一个标量，所以调用backward()时不需要指定求导变量：\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学上，如果有一个函数值和自变量都为向量的函数$\\vec{y}=f(\\vec{x})$, 那么$\\vec{y}$关于 $\\vec{x}$的梯度就是一个雅可比矩阵（Jacobian matrix）:\n",
    "\n",
    "$$J=\\left(\\begin{array}{ccc}\\frac{\\partial y_{1}}{\\partial x_{1}}  \\cdots \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\\vdots  \\ddots  \\vdots\\\\\\frac{\\partial y_{m}}{\\partial x_{1}}  \\cdots  \\frac{\\partial y_{m}}{\\partial x_{n}}\\end{array}\\right)$$       \n",
    "\n",
    "```torch.autograd```这个包就是用来计算一些雅克比矩阵的乘积的。例如$v$是一个标量函数$l=g(\\vec{y})$的梯度:     \n",
    "$$v = (\\frac{\\partial l}{\\partial y_1},\\cdots,\\frac{\\partial l}{\\partial y_m})$$     \n",
    "\n",
    "那么根据链式法则我们有$l$关$\\vec{x}$的雅克比矩阵就为:    \n",
    "\n",
    "$$v J=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}}  \\cdots  \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right) \\left(\\begin{array}{ccc}\\frac{\\partial y_{1}}{\\partial x_{1}}  \\cdots  \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\\vdots  \\ddots \\vdots\\\\\\frac{\\partial y_{m}}{\\partial x_{1}}  \\cdots  \\frac{\\partial y_{m}}{\\partial x_{n}}\\end{array}\\right)=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial x_{1}}  \\cdots \\frac{\\partial l}{\\partial x_{n}}\\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"red\">**注意：grad在反向传播过程中是累加的(accumulated)**，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般**在反向传播之前需把梯度清零**。<\\font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:33.886977Z",
     "start_time": "2022-01-28T08:48:33.873014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.5000, 5.5000],\n",
      "        [5.5000, 5.5000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 再来反向传播一次，注意grad是累加的\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "\n",
    "x.grad.data.zero_()\n",
    "\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **为什么在y.backward()时，如果y是标量，则不需要为backward()传入任何参数；否则，需要传入一个与y同形的Tensor?**         \n",
    "\n",
    "简单来说就是为了**避免向量（甚至更高维张量）对张量**求导，而**转换成标量对张量**求导。        \n",
    "\n",
    "举个例子，假设形状为```m × n```的矩阵```X```经过运算得到了```p × q```的矩阵 ```Y```，```Y ```又经过运算得到了```s × t```的矩阵```Z```。     \n",
    "\n",
    "那么按照前面讲的规则，```dZ/dY```应该是一个```s×t×p×q```四维张量，```dY/dX```是一个```p×q×m×n```的四维张量。         \n",
    "\n",
    "- **存在问题**\n",
    "1. 两个四维张量难以相乘\n",
    "2. 四维和三维张量难以相乘  \n",
    "3. 导数的导数在此种情况下难以求出     \n",
    "\n",
    "\n",
    "- **解决方法**:不允许张量对张量求导，只允许标量对张量求导，求导结果是和自变量同形的张量,所以必要时我们要把张量通过将所有张量的元素加权求和的方式转换为标量。\n",
    "\n",
    "\n",
    "举个例子，假设$y$由自变量$x$计算而来，$w$是和$y$同形的张量，则$y.backward(w)$的含义是：先计算$l = torch.sum(y\\times w)$，则$l$是个标量，然后求$l$对自变量$x$的导数，$dl/dx = d(y*w)/dx$。      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:33.902934Z",
     "start_time": "2022-01-28T08:48:33.888972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward0>)\n",
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2, 2)\n",
    "print(z)\n",
    "\n",
    "\n",
    "# 现在 z 不是一个标量，所以在调用backward时需要传入一个和z同形的权重向量进行加权求和得到一个标量。\n",
    "\n",
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**中断梯度追踪的例子**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:33.918895Z",
     "start_time": "2022-01-28T08:48:33.904929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2 \n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad) # True\n",
    "print(y2, y2.requires_grad) # False\n",
    "print(y3, y3.requires_grad) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:33.934849Z",
     "start_time": "2022-01-28T08:48:33.924878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "y3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们想要修改```tensor```的数值，但是又不希望被```autograd```记录（即不会影响反向传播），那么我么可以对```tensor.data```进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T08:48:33.950806Z",
     "start_time": "2022-01-28T08:48:33.935846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # False,已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
