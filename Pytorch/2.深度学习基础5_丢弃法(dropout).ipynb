{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习基础5_丢弃法(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\torch\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda\\envs\\torch\\lib\\site-packages\\numpy\\.libs\\libopenblas.JPIJNSWNNAN3CE6LLI5FWSPHUT2VXMTH.gfortran-win_amd64.dll\n",
      "D:\\Anaconda\\envs\\torch\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "## 定义dropout\n",
    "def dropout(X,drop_prob):\n",
    "    X = X.float()\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # 元素全部丢弃  \n",
    "    if keep_prob == 0:\n",
    "        return torch.zeros_like(X)\n",
    "    mask = (torch.rand(X.shape) < keep_prob).float()\n",
    "    \n",
    "    return mask * X / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1333, 0.2667, 0.4000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.0667, 0.0000, 0.0000, 0.0000, 1.6000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = (torch.arange(16)/15).view(2, 8)\n",
    "dropout(X, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = torch.tensor(np.random.normal(0, 0.01, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=True)\n",
    "b1 = torch.zeros(num_hiddens1, requires_grad=True)\n",
    "W2 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=True)\n",
    "b2 = torch.zeros(num_hiddens2, requires_grad=True)\n",
    "W3 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=True)\n",
    "b3 = torch.zeros(num_outputs, requires_grad=True)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1,drop_prob2 = 0.2 , 0.5\n",
    "def net(X,is_training = True):\n",
    "    X = X.view(-1,num_inputs)\n",
    "    H1 = (torch.matmul(X,W1)+b1).relu()\n",
    "    # 只在训练模型时使用丢弃法\n",
    "    if is_training:\n",
    "        H1 = dropout(H1,drop_prob1)\n",
    "        # 在第一层全连接后添加丢弃层\n",
    "        \n",
    "    H2 = (torch.matmul(H1, W2) + b2).relu()\n",
    "    if is_training:\n",
    "        H2 = dropout(H2, drop_prob2)  \n",
    "        # 在第二层全连接后添加丢弃层\n",
    "        \n",
    "    return torch.matmul(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`isinstance()` 函数来判断一个对象是否是一个已知的类型，类似 `type()`。\n",
    "\n",
    "- `isinstance()` 与 `type()` 区别：\n",
    "    - `type()` 不会认为子类是一种父类类型，不考虑继承关系。\n",
    "    - `isinstance()` 会认为子类是一种父类类型，考虑继承关系。\n",
    "如果要判断两个类型是否相同推荐使用 isinstance()。\n",
    "\n",
    "\n",
    "- **语法:**       \n",
    "`isinstance(object, classinfo)`\n",
    "\n",
    "参数:\n",
    "```\n",
    "object -- 实例对象。\n",
    "classinfo -- 可以是直接或间接类名、基本类型或者由它们组成的元组。\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "def evaluate_accuracy(data_iter,net):\n",
    "    acc_sum,n = 0.0, 0\n",
    "    for X,y in data_iter:\n",
    "        if isinstance(net,torch.nn.Module):\n",
    "            net.eval()# 评估模式，关闭dropout   \n",
    "            acc_sum += (net(X).argmax(dim = 1) == y).float().sum().item()\n",
    "            net.train()# 训练模式   \n",
    "        else:# 自定义模型\n",
    "            # func.__code__.co_argcount：返回函数的参数个数，参数个数不包含*args与**kwargs\n",
    "            if(\"is_training\" in net.__code__.co_varnames):\n",
    "                # 将is_training设置成False\n",
    "                acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "            else:\n",
    "                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0045, train acc 0.551, test acc 0.722, epoch time 8.1314\n",
      "epoch 2, loss 0.0023, train acc 0.788, test acc 0.782, epoch time 7.9757\n",
      "epoch 3, loss 0.0019, train acc 0.822, test acc 0.821, epoch time 7.8871\n",
      "epoch 4, loss 0.0017, train acc 0.839, test acc 0.833, epoch time 8.5571\n",
      "epoch 5, loss 0.0016, train acc 0.848, test acc 0.789, epoch time 8.3320\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size = 5, 100.0, 256\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropout简洁实现   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0044, train acc 0.566, test acc 0.778, epoch time 8.0185\n",
      "epoch 2, loss 0.0022, train acc 0.789, test acc 0.798, epoch time 8.0809\n",
      "epoch 3, loss 0.0019, train acc 0.819, test acc 0.793, epoch time 8.1905\n",
      "epoch 4, loss 0.0017, train acc 0.841, test acc 0.800, epoch time 8.1412\n",
      "epoch 5, loss 0.0016, train acc 0.849, test acc 0.846, epoch time 8.0834\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "        d2l.FlattenLayer(),\n",
    "        nn.Linear(num_inputs, num_hiddens1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Linear(num_hiddens1, num_hiddens2), \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Linear(num_hiddens2, 10)\n",
    "        )\n",
    "\n",
    "for param in net.parameters():\n",
    "    nn.init.normal_(param, mean=0, std=0.01)\n",
    "    \n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a5f714b4f6e5a2acf9ea4e7b1c0c43f376d2284ed3b59922760a9b4b748ea21"
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
